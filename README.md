# ğŸš€ Databricks Pipelines

This repository contains modular data pipelines built using **Databricks**, **Azure Blob Storage**, **Delta Lake**, and **Workflows**.

The goal is to explore multiple ways to handle batch ingestion and processing using clean, cost-effective patterns that can scale to streaming with Autoloader in future iterations.

---

## ğŸ“¦ Contents

databricks-pipelines/
â”œâ”€â”€ pipeline1_batch_delta/
â”‚ â”œâ”€â”€ ingest_source_a.py
â”‚ â”œâ”€â”€ ingest_source_b.py
â”‚ â”œâ”€â”€ transform_and_merge.py
â”‚ â””â”€â”€ write_to_delta.py
â”œâ”€â”€ common/
â”‚ â””â”€â”€ utils.py # Reusable helper functions (future)
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ mock_source_a/ # Sample CSV data (Azure-style)
â”‚ â””â”€â”€ mock_source_b/ # Sample JSON data

## ğŸ” Pipeline Variants (Planned)

| Pipeline                     | Features                                     |
|-----------------------------|----------------------------------------------|
| `pipeline1_batch_delta`     | Batch ingest â†’ Delta write â†’ Workflow-based |
| `pipeline2_modular_functions` | Shared function logic, reusable modules     |
| `pipeline3_autoloader_batch` | Uses Autoloader with manual trigger         |
| `pipeline4_streaming_mode`  | Future: Continuous ingestion with streaming |

## ğŸ§° Technologies

- Databricks Runtime 15.4
- Delta Lake
- Azure Blob Storage
- PySpark
- Databricks Workflows
- GitHub integration

  ## ğŸ§ª Testing

Mock data is stored under `/data/` folders and versioned for testing ingestion logic and transformations.

## ğŸ§  Goals

- Practice modular pipeline design using functions
- Compare approaches to batch ingestion
- Scale to Autoloader and Streaming
- Keep costs low (target: <$50/month)

## ğŸ”’ License

This project is open source under the MIT license.
