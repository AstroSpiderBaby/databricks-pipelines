# ğŸš€ Databricks Pipelines

This repository contains modular data pipelines built using **Databricks**, **Azure Blob Storage**, **Delta Lake**, and **Workflows**.

The goal is to explore multiple ways to handle batch ingestion and processing using clean, cost-effective patterns that can scale to streaming with Autoloader in future iterations.

---

## ğŸ“¦ Project Structure

databricks-pipelines/
â”œâ”€â”€ pipeline1_batch_delta/
â”‚   â”œâ”€â”€ bronze/       # Raw ingestion notebooks (source_a, source_b, etc.)
â”‚   â”œâ”€â”€ silver/       # Transformation and data quality logic
â”‚   â”œâ”€â”€ gold/         # Aggregated business-ready outputs
â”‚   â”œâ”€â”€ transform/    # Merge logic and shared transformations
â”‚   â”œâ”€â”€ utils/        # Mounting, write helpers, and reusable scripts
â”‚   â””â”€â”€ docs/         # Setup, Key Vault, Blob mount instructions
â”œâ”€â”€ common/           # Shared utilities across pipelines

## ğŸ” Pipeline Variants (Planned)

| Pipeline                     | Features                                     |
|-----------------------------|----------------------------------------------|
| `pipeline1_batch_delta`     | Batch ingest â†’ Delta write â†’ Workflow-based |
| `pipeline2_modular_functions` | Shared function logic, reusable modules     |
| `pipeline3_autoloader_batch` | Uses Autoloader with manual trigger         |
| `pipeline4_streaming_mode`  | Future: Continuous ingestion with streaming |

## ğŸ§° Technologies

- Databricks Runtime 15.4
- Delta Lake
- Azure Blob Storage
- PySpark
- Databricks Workflows
- GitHub integration

  ## ğŸ§ª Testing

Mock data is stored under `/data/` folders and versioned for testing ingestion logic and transformations.

## ğŸ§  Goals

- Practice modular pipeline design using functions
- Compare approaches to batch ingestion
- Scale to Autoloader and Streaming
- Keep costs low (target: <$50/month)

## ğŸ”’ License

This project is open source under the MIT license.
