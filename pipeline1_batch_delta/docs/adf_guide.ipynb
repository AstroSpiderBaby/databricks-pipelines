{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7a18f05-49e7-4c33-aeff-bbff27f28cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üöÄ Azure Data Factory Pipeline Guide: Ingest, Enrich & Append Vendor Registry\n",
    "This guide outlines how to use ADF to ingest a Parquet dataset from Azure Blob Storage, enrich it with metadata, and prepare it for downstream consumption in Databricks.\n",
    "\n",
    "üìÇ 1. Linked Service Setup\n",
    "Go to Manage > Linked Services\n",
    "\n",
    "Create a new linked service:\n",
    "\n",
    "Type: Azure Blob Storage\n",
    "\n",
    "Name: is_blob\n",
    "\n",
    "Auth Method: RBAC with Managed Identity OR Storage Account Key\n",
    "\n",
    "Storage account: Select your target (e.g., datalakelv426)\n",
    "\n",
    "üìÅ 2. Datasets Setup\n",
    "Create 2 datasets:\n",
    "\n",
    "Source Dataset\n",
    "\n",
    "Type: Parquet\n",
    "\n",
    "Linked Service: is_blob\n",
    "\n",
    "File path: Container/folder (e.g., pipeline1_batch_delta/vendor_registry.parquet)\n",
    "\n",
    "Name: ds_vendor_registry\n",
    "\n",
    "Sink Dataset\n",
    "\n",
    "Type: Parquet (or Delta if using Synapse/SQL sink)\n",
    "\n",
    "Linked Service: Same as above\n",
    "\n",
    "File path: Output folder (e.g., delta/silver/vendor_registry_enriched)\n",
    "\n",
    "Name: ds_vendor_registry_enriched\n",
    "\n",
    "üîÑ 3. Create Data Flow\n",
    "Go to Author > Data Flows > New Data Flow\n",
    "\n",
    "üí° Steps inside Data Flow:\n",
    "Source\n",
    "\n",
    "Use ds_vendor_registry\n",
    "\n",
    "Ensure schema projection works\n",
    "\n",
    "Set Projection to Allow schema drift if needed\n",
    "\n",
    "Select Columns (optional)\n",
    "\n",
    "Use Select transformation to rename and reorder columns\n",
    "\n",
    "Example:\n",
    "\n",
    "Rename reg_date ‚Üí registration_date\n",
    "\n",
    "Keep vendor_id, vendor_name, etc.\n",
    "\n",
    "Derived Columns\n",
    "Add 3 new metadata columns:\n",
    "\n",
    "ingested_at: currentUTC()\n",
    "\n",
    "source_tag: \"vendor_registry_adf\"\n",
    "\n",
    "pipeline_run_id: concat(currentUTC(), '-', $pipelineRunId)\n",
    "\n",
    "Sink\n",
    "\n",
    "Use ds_vendor_registry_enriched\n",
    "\n",
    "Mode: Append\n",
    "\n",
    "Enable Auto Mapping\n",
    "\n",
    "‚öôÔ∏è 4. Add Pipeline to Trigger the Flow\n",
    "Create a Pipeline\n",
    "\n",
    "Add a Data Flow Activity\n",
    "\n",
    "Link to your created data flow\n",
    "\n",
    "Pass $pipelineRunId as a parameter\n",
    "\n",
    "üß™ 5. Validate, Publish, Trigger\n",
    "Click Validate All\n",
    "\n",
    "Fix any projection or expression errors\n",
    "\n",
    "Click Publish All\n",
    "\n",
    "Click Trigger Now or schedule as needed\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "adf_guide",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
