{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952c8f0a-3a1e-4df7-92a4-2f8250199dd9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1751585678320}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, upper, trim, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Step 1: Load Bronze\n",
    "df = spark.read.format(\"delta\").load(\"/mnt/delta/bronze/finance_invoices\")\n",
    "\n",
    "\n",
    "# Step 2: Clean columns\n",
    "df_clean = (\n",
    "    df.withColumn(\"vendor\", upper(trim(col(\"vendor\"))))\n",
    "      .withColumn(\"invoice_date\", to_date(col(\"invoice_date\")))\n",
    "      .withColumn(\"due_date\", to_date(col(\"due_date\")))\n",
    "      .withColumn(\"paid_flag\", when(col(\"paid\") == \"Yes\", 1).otherwise(0).cast(IntegerType()))\n",
    "      .drop(\"paid\", \"vendor_name\", \"rating\", \"location\")  # ❌ drop unreliable fields\n",
    ")\n",
    "\n",
    "# Step 3: Create vendor mapping DataFrame\n",
    "\n",
    "# Create mapping DataFrame\n",
    "vendor_map = [\n",
    "    (\"WOLFE LLC\", \"V010\"),\n",
    "    (\"MOORE-BERNARD\", \"V008\"),\n",
    "    (\"GARCIA-JAMES\", \"V006\"),\n",
    "    (\"ABBOTT-MUNOZ\", \"V001\"),\n",
    "    (\"BLAIR PLC\", \"V003\"),\n",
    "    (\"DUDLEY GROUP\", \"V004\"),\n",
    "    (\"ARNOLD LTD\", \"V002\"),\n",
    "    (\"MCCLURE, WARD AND LEE\", \"V007\"),\n",
    "    (\"WILLIAMS AND SONS\", \"V009\"),\n",
    "    (\"GALLOWAY-WYATT\", \"V005\"),\n",
    "]\n",
    "\n",
    "df_map = spark.createDataFrame(vendor_map, [\"vendor\", \"vendor_id\"])\n",
    "\n",
    "# Clean original vendor names to ensure match\n",
    "df_enriched = df_clean.withColumn(\"vendor\", upper(trim(col(\"vendor\"))))\n",
    "\n",
    "# Join on cleaned vendor name\n",
    "df_final = df_enriched.join(df_map, on=\"vendor\", how=\"inner\")\n",
    "\n",
    "# Step 6: Reorder columns\n",
    "df_final = df_final.select(\n",
    "    \"vendor_id\", \"invoice_id\", \"amount_usd\", \"invoice_date\", \"due_date\",\n",
    "    \"paid_flag\", \"source_file\", \"ingestion_type\"\n",
    ")\n",
    "display(df_final)\n",
    "\n",
    "# Step 7: Overwrite Silver path\n",
    "output_path = \"dbfs:/mnt/delta/silver/finance_invoices_v2\"\n",
    "dbutils.fs.rm(output_path, recurse=True)\n",
    "\n",
    "try:\n",
    "    df_final.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .partitionBy(\"invoice_date\") \\\n",
    "        .save(output_path)\n",
    "    print(\"✅ Write successful.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Write failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f90112-15bb-4a71-a361-0aebe303a649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a34d00e9-43a5-485a-ac10-4ea387b5d97a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.format(\"delta\").load(\"/mnt/delta/bronze/finance_invoices\")\n",
    "\n",
    "# df = df.withColumn(\"invoice_date\", to_date(col(\"invoice_date\")))\n",
    "\n",
    "# df.filter(\"invoice_date IS NULL\").show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_finance_invoices.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
