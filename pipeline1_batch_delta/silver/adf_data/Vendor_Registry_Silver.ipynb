{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f14402b-e2e2-406f-85ff-ba565ee67496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Repos/brucejenks@live.com/databricks-pipelines/pipeline1_batch_delta/utils/utils_upsert_with_hashstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19240a9-6ea6-4609-a302-418d037d8ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load raw vendor registry (output from ADF)\n",
    "df_raw = spark.read.format(\"parquet\").load(\"dbfs:/mnt/adf-silver\")\n",
    "\n",
    "\n",
    "\n",
    "# Select and clean necessary fields\n",
    "df_clean = df_raw.select(\n",
    "    \"vendor_id\", \"industry\", \"headquarters\", \"onwatchlist\",\n",
    "    \"registrationdate\", \"tier\", \"ingestion_timestamp\", \"source_tag\"\n",
    ")\n",
    "\n",
    "# Define target cleaned Delta table\n",
    "target_path = \"/mnt/delta/silver/vendor_registry_clean\"\n",
    "\n",
    "# Define deduplication key\n",
    "unique_keys = [\"vendor_id\"]  # Can add registrationdate or other compound key if needed\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, target_path):\n",
    "    DeltaTable.forPath(spark, target_path).toDF().unpersist()\n",
    "    dbutils.fs.rm(target_path, recurse=True)\n",
    "    print(f\"Dropped old target at {target_path}\")\n",
    "\n",
    "# Upsert using utility\n",
    "upsert_with_hashstring(\n",
    "    spark=spark,\n",
    "    df=df_clean,\n",
    "    target_path=target_path,\n",
    "    unique_cols=unique_keys,\n",
    "    hash_col_name=\"hashstring\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Vendor_Registry_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
