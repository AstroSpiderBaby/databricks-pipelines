{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43558f0f-11b0-4d25-adc0-555dd029e7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "from typing import List, Optional\n",
    "\n",
    "def upsert_with_hashstring(\n",
    "    spark: SparkSession,\n",
    "    df: DataFrame,\n",
    "    target_path: str,\n",
    "    unique_cols: List[str],\n",
    "    partition_cols: Optional[List[str]] = None,\n",
    "    hash_col_name: str = \"hashstring\",\n",
    "    register_table: bool = True,\n",
    "    dry_run: bool = False,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upserts a DataFrame to a Delta table using a hashstring as a deduplication key.\n",
    "    Also supports schema printing, dry-run mode, partitioning, and table registration.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Spark session\n",
    "        df (DataFrame): Source DataFrame to upsert\n",
    "        target_path (str): Path to Delta table\n",
    "        unique_cols (List[str]): Columns to use to build hashstring\n",
    "        partition_cols (List[str], optional): Columns to partition by\n",
    "        hash_col_name (str): Name of the hash column (default 'hashstring')\n",
    "        register_table (bool): Whether to register table in metastore\n",
    "        dry_run (bool): If True, do not perform write, just log actions\n",
    "        verbose (bool): If True, print detailed logging and schema\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Add hash column for deduplication\n",
    "    df_hashed = df.withColumn(hash_col_name, sha2(concat_ws(\"||\", *unique_cols), 256))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üß† Generated hashstring from columns: {unique_cols}\")\n",
    "        df_hashed.select(hash_col_name).show(truncate=False)\n",
    "        df_hashed.printSchema()\n",
    "        print(f\"üóÇÔ∏è Target Path: {target_path}\")\n",
    "\n",
    "    # Step 2: Perform dry-run preview\n",
    "    if dry_run:\n",
    "        print(\"üö´ Dry run mode ‚Äî skipping write operation.\")\n",
    "        return\n",
    "\n",
    "    # Step 3: If Delta table exists, perform merge (deduplication)\n",
    "    if DeltaTable.isDeltaTable(spark, target_path):\n",
    "        if verbose:\n",
    "            print(\"üîÅ Delta table exists ‚Äî performing MERGE (upsert).\")\n",
    "\n",
    "        delta_table = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df_hashed.alias(\"source\"),\n",
    "            f\"target.{hash_col_name} = source.{hash_col_name}\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"üÜï No Delta table found ‚Äî writing new Delta table.\")\n",
    "\n",
    "        writer = df_hashed.write.format(\"delta\").mode(\"overwrite\")\n",
    "\n",
    "        if partition_cols:\n",
    "            writer = writer.partitionBy(partition_cols)\n",
    "\n",
    "        writer.save(target_path)\n",
    "\n",
    "    # Step 4: Register table in metastore (optional)\n",
    "    if register_table:\n",
    "        table_name = target_path.rstrip(\"/\").split(\"/\")[-1]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{target_path}'\n",
    "        \"\"\")\n",
    "        if verbose:\n",
    "            print(f\"üìö Table registered: {table_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils_upsert_with_hashstring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
