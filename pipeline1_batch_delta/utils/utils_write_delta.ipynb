{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21fc989c-6bdd-4684-8e80-b8cd9d2f9b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "write_utils.py\n",
    "\n",
    "Contains helper functions for writing Spark DataFrames to Delta Lake with standardized configurations.\n",
    "Promotes consistent use of overwrite behavior, schema merging, and partitioning across the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "def write_to_delta(df, path, mode=\"overwrite\", partition_by=None, merge_schema=True):\n",
    "    \"\"\"\n",
    "    Writes a Spark DataFrame to Delta Lake format.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The DataFrame to write.\n",
    "    path : str\n",
    "        The target Delta Lake path.\n",
    "    mode : str, optional\n",
    "        Save mode, default is 'overwrite'.\n",
    "    partition_by : str or list[str], optional\n",
    "        Column(s) to partition by.\n",
    "    merge_schema : bool, optional\n",
    "        Whether to merge schema during write.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3b7610f-2ab5-410f-86d4-7811f2940ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# /utils/write_utils\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import Optional, List\n",
    "\n",
    "def write_df_to_delta(\n",
    "    df: DataFrame,\n",
    "    path: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    register_table: bool = True,\n",
    "    partition_by: Optional[List[str]] = None,\n",
    "    dry_run: bool = False,\n",
    "    verbose: bool = False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Writes a Spark DataFrame to Delta Lake with optional schema merging, table registration, and partitioning.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write\n",
    "    - path (str): Destination path (e.g., /mnt/delta/bronze/table_name)\n",
    "    - mode (str): Write mode ('overwrite', 'append', etc.)\n",
    "    - merge_schema (bool): Enable mergeSchema (default True)\n",
    "    - register_table (bool): Register as Hive/Unity Catalog table (default True)\n",
    "    - partition_by (list[str], optional): List of columns to partition by\n",
    "    - dry_run (bool): If True, does not actually write ‚Äî just prints what would happen\n",
    "    - verbose (bool): If True, prints schema, path, and action details\n",
    "\n",
    "    Returns:\n",
    "    - str: Table name if registered, otherwise None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if verbose:\n",
    "            print(f\"üìÅ Write Path: {path}\")\n",
    "            print(f\"üìù Write Mode: {mode}\")\n",
    "            print(f\"üîÄ Partitioning: {partition_by}\")\n",
    "            print(f\"üìä Schema:\")\n",
    "            df.printSchema()\n",
    "\n",
    "        writer = df.write.format(\"delta\").mode(mode)\n",
    "\n",
    "        if merge_schema:\n",
    "            writer = writer.option(\"mergeSchema\", \"true\")\n",
    "\n",
    "        if partition_by:\n",
    "            writer = writer.partitionBy(partition_by)\n",
    "\n",
    "        if not dry_run:\n",
    "            writer.save(path)\n",
    "            if verbose:\n",
    "                print(f\"‚úÖ Data written to {path}\")\n",
    "\n",
    "        table_name = path.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "        if register_table:\n",
    "            spark = SparkSession.builder.getOrCreate()\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE {table_name}\n",
    "                USING DELTA\n",
    "                LOCATION '{path}'\n",
    "            \"\"\")\n",
    "            if verbose:\n",
    "                print(f\"üìö Table registered: {table_name}\")\n",
    "\n",
    "            return table_name\n",
    "\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error writing to Delta: {e}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils_write_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
