{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21fc989c-6bdd-4684-8e80-b8cd9d2f9b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "write_utils.py\n",
    "\n",
    "Contains helper functions for writing Spark DataFrames to Delta Lake with standardized configurations.\n",
    "Promotes consistent use of overwrite behavior, schema merging, and partitioning across the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "def write_to_delta(df, path, mode=\"overwrite\", partition_by=None, merge_schema=True):\n",
    "    \"\"\"\n",
    "    Writes a Spark DataFrame to Delta Lake format.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The DataFrame to write.\n",
    "    path : str\n",
    "        The target Delta Lake path.\n",
    "    mode : str, optional\n",
    "        Save mode, default is 'overwrite'.\n",
    "    partition_by : str or list[str], optional\n",
    "        Column(s) to partition by.\n",
    "    merge_schema : bool, optional\n",
    "        Whether to merge schema during write.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3b7610f-2ab5-410f-86d4-7811f2940ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# /utils/write_utils\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def write_df_to_delta(df, path, mode=\"overwrite\", merge_schema=True, register_table=True):\n",
    "    \"\"\"\n",
    "    Generic Delta writer for any DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Spark DataFrame to write\n",
    "    - path: Destination path (e.g., /mnt/delta/bronze/...) \n",
    "    - mode: Write mode (default is \"overwrite\")\n",
    "    - merge_schema: Whether to enable mergeSchema (default True)\n",
    "    - register_table: Whether to register a Hive table at same location\n",
    "    \"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    writer.save(path)\n",
    "\n",
    "    if register_table:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        table_name = path.rstrip(\"/\").split(\"/\")[-1]  # last folder = table name\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{path}'\n",
    "        \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "write_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
