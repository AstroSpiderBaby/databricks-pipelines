"""
utils_write_silver.py

Silver-level Delta write utility using hash-based upsert.
"""

import os
from pyspark.sql import DataFrame
from pyspark.sql.utils import AnalysisException
from utils_py.utils_upsert_with_hashstring import upsert_with_hashstring  # âœ… FIXED import


def write_silver_upsert(
    df: DataFrame,
    path: str,
    full_table_name: str,
    primary_key: str,
    hash_col: str = "hashstring",
    required_columns: list[str] = None,
    partition_by: list[str] = None,
    register_table: bool = True,
    verbose: bool = False
):
    spark = df.sparkSession

    # âœ… Column validation
    if required_columns:
        actual = set(df.columns)
        missing = set(required_columns) - actual
        if missing:
            raise ValueError(f"Missing required columns: {missing}")

    if verbose:
        print(f"\nğŸ§ª Columns: {df.columns}")
        print(f"ğŸ“ Writing to: {path}")
        print(f"ğŸ”‘ Primary key: {primary_key}")
        print(f"ğŸ“¦ Partitioning: {partition_by or 'None'}")

    # âœ… Step 1: Perform upsert
    try:
        upsert_with_hashstring(
            df_new=df,
            path=path,
            primary_key=primary_key,
            hash_col=hash_col
        )
    except Exception as e:
        raise RuntimeError("ğŸ”¥ Upsert failed inside write_silver_upsert.") from e

    # âœ… Step 2: Register Unity Catalog table dynamically
    if register_table and path.startswith("/Volumes/"):
        try:
            spark.sql(f"""
                CREATE TABLE IF NOT EXISTS {full_table_name}
                AS SELECT * FROM delta.`{path}`
            """)
            if verbose:
                print(f"ğŸ“š Registered Unity Catalog table: {full_table_name}")
        except Exception as e:
            print(f"âš ï¸ Failed to register table {full_table_name}: {e}")


    if verbose:
        print("âœ… Silver table write complete.")
